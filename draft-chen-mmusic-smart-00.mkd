---
title: Simple Media Annotation for Real-Time comunication
abbrev: SMART
docname: draft-chen-mmusic-smart-00.txt
date: 2018-10-10

# stand_alone: true

ipr: trust200902
area: ART
wg: mmusic
kw: Internet-Draft
cat: std

coding: us-ascii
pi:    # can use array (if all yes) or hash here
  toc: yes
  sortrefs: yes
  symrefs: yes

author:
      -
        ins: W. Chen
        name: Wei Chen
        org: CVTE
        street: 2 YouZhi Nong, Road MaoJiaQiao 
        city: HangZhou
        code: 310012
        country: China
        phone: +86-013858060444
        email: oeichenwei@gmail.com
      -
        ins: J. Fu
        name: Jiantao Fu
        org: Cisco Systems, Inc.
        street: 771 Alder Dr
        city: Milpitas
        code: 95035
        country: USA
        phone: "+1-510-403-5644"
        email: jianfu@cisco.com

normative:
  RFC6838:
  RFC4855:
  RFC3550:


--- abstract

While AI technology arising, automatical video annotation, speech transcription
and similar such technologies are becoming mature. This document proposes one solution 
to transfer and sync such meta data along the media in the context of RTCWeb.
 
--- middle 
# Introduction

  Nowdays, face tracking and face recognition have already made a great progress
and are widely adopted. Face tracking can be used to smart cropping of the video avoiding 
the face being cut, face recognition can be used to identify who are engaged in the 
communication by labelling the name in the video. Speech recognition technology 
becomes mature too, it is not surprised that speech can be automatically 
transcribed to text and assist the real time communications. AI technology can be applied 
to other use cases, like object tracking, emotion detecting, simultaneous translation and so on.

~~~~~~~~~~~
                        +-----------------+
            +---------> | Media Processor +------------+
            |           +-----------------+            |
            |                                          |
            |                                          |
            |                                          |
            |                                          |
        +----+----+                               +-----v----+
        | Sender  |                               | Receiver |
        +---------+                               +----------+
~~~~~~~~~~~
{: #center-topo title="Media Flow Topology" }
                          
  In a real-time communication scenario like in {{center-topo}}, these intellegent 
processing can be done at sender, media processor or receiver. 
The advantage of doing it at receiver or media processor was backward 
compatibility but there are also disadvantages:

  1. the media quality might degrade due to packet loss or delay on the network.

  2. either all receivers needs to repeat the computation or extra server 
     resources are required to compute it.

  3. some meta data could be captured only on source side, for example, 
     iPhone 3D camera could help extract more information than pure image 
     processing.
  
This document will focus on methods to capture the media meta data at sender 
side and define how to transfer them along with media data to the receiver.

Mixing the meta data with media at the sender is one way to convey them to receivers
, but there are some drawbacks, take compositing annotation text onto the source
picture as an example:
  
  1. If the video was degraded due to network, the text will be unclear.
  
  2. The receiver can not turn on/off the annotation on-demand.
  
  3. The receiver can not customize the utilization of such meta data.
  
This document proposes a new out-of-band payload to transfer them and synchornize
them through timestamp at receiver, rather than extending the current media 
payloads:

  1. It is easier to achieve backward compatiblity.
  
  2. The transportation of the meta data could be different from the media.
     For example, the face tracking information could be combined with the audio 
     transcribed text and render together.

The mechanism described in this document can also be used to transfer other 
media meta data. For example, for dekstop sharing case, the mouse movement events
can be transfered separately in this format, and we can achieve higher 
frame rate of mouse movement than the screen content.

# Message Format

~~~~~~~~~~~
                    +---------------+---------------+
                    |0|1|2|3|4|5|6|7|0|1|2|3|4|5|6|7|
                    +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
                    | V |   RES | P |      Count    |
                    +---+-------+---+---------------+
~~~~~~~~~~~
{: #message-common-hdr title="Common Message Header" }

The message will have 2 bytes header followed with an array of sub-messages. 
All sub-messages share the same common header as in {{message-common-hdr}}.

V:
: Version. The version of this message, it should be set to 0.

RES:
: Reserved. The reserved field, should be set to 0 now.

P:
: Priority. The priority of the message, lower priority packets can be dropped or delayed.

Count:
: Count. The number of sub-messages embedded. There at least one submessage embedded, 
  so the count plus one is the actual number of sub-messages.

## Sub-message common header

~~~~~~~~~~~
      +---------------+---------------+---------------+---------------+
      |0|1|2|3|4|5|6|7|0|1|2|3|4|5|6|7|0|1|2|3|4|5|6|7|0|1|2|3|4|5|6|7|
      +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
      |     Type      |     Len       |              SSRC             |
      +---------------+---------------+---------------+---------------+
      |0|1|2|3|4|5|6|7|0|1|2|3|4|5|6|7|0|1|2|3|4|5|6|7|0|1|2|3|4|5|6|7|
      +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
      |            ...SSRC            |          Timestamp            |
      +---------------+---------------+---------------+---------------+
      |       ...Timestamp            |
      +---------------+---------------+
~~~~~~~~~~~
{: #submessage-fmt title="General Sub-message Format" }

Each sub-message will have at least 10 bytes headers, the details are:

Type:
: one-byte pre-defined sub-message type, see table 1.
    
Len:
: the length of the sub-message, not including the common header, in bytes

SSRC:
: 4 bytes, the SSRC of the corresponding video stream even when this payload 
  is generated from an audio stream. The annotation should be rendered to the 
  corresponding video stream.

Timestamp:
: 4 bytes, the same [RFC3550] RTP timestamp space as the corresponding video stream.

This document defines 1 sub-message type and they can be extended in future.

~~~~~~~~~~~
                +------------------+----------+
                |    Sub-message   |   Value  |  
                +------------------+----------+
                |  SMART_RECTS     |     0    |
                +------------------+----------+
~~~~~~~~~~~
{: #message-type title="Pre-defined Message Types" }

## SMART_RECTS

~~~~~~~~~~~
      +---------------+---------------+---------------+---------------+
      |0|1|2|3|4|5|6|7|0|1|2|3|4|5|6|7|0|1|2|3|4|5|6|7|0|1|2|3|4|5|6|7|
      +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
      |S|I|P|T|  RES  |   Name Len    |               ID              |
      +---------------+---------------+---------------+---------------+
      |             Left              |             Top               |
      +---------------+---------------+---------------+---------------+
      |             Width             |            Height             |
      +---------------+---------------+---------------+---------------+
      |          Time offset          |            Name...            |
      +---------------+---------------+---------------+---------------+
~~~~~~~~~~~      
{: #message-rects title="SMART_RECTS Message Format" }

This pattern can be repeated, the total number of rectangles could be determined
from the length field of the sub-message common header. Each filed of this 
sub-message is defined as:

S:
: controls the Width/Height fields. If it is 0, there is no Width/Height.

I:
: controls the ID field. If it is 0, there is no ID filed.

P:
: synchornization policy. Zero means it will be rendered as soon as possible when
  the message was received and one means it will be synchronized with video stream,
  please refer to "Synchronize with Media" section for detail. 

T:
: controls the Time offset field. If it is 0, there is no time offset field.

RES:
: Reserved field, must be set to zero.

Name Len:
: the length of the name field, it can between 0 - 255.

ID:
: the identifier of the rectangle, value is defined by the app.

Left/Top:
: 2 bytes each, the left top point of the rectangle.

Width/Height:
: 2 bytes each, the width and height of the rectangle.

Time offset:
: the timestamp offset to the common header timestamp. This is useful when there is 
  multiple submessages in one message to save the bandwidth. The unit is the same
  as video RTP timestamp.

Name:
: the null-terminated UTF-8 string of the name tagging that rectangle, up to 255 bytes.

# SDP Negotiation

The message can be sent along with RTP stream as RTP payload, it can also be 
sent through Data Channel in SCTP.

When it is sent through RTP stream, it can be multiplexed with either Audio or 
Video stream with its own sequence space and separate SSRC, the payload type 
can be negotiated with dynamic payloads.

Resilience technologies like FEC and RTP retransmission can be used on the
annotation stream to provide some kind of reliablity when packet loss. Or in
separate RTP session over realiabe channel like SCTP or TCP. 

## Media Type Registration

Media type registration is done according to [RFC6838] and [RFC4855].

Type name:
: application

Subtype name:
: annotation

Required parameters:
:    
    type:
    : supported sub-message type values, use vertical bar to concatenate, 
      for example, type=0|1, means it will generate both SMART_RECTS and 
      SMART_TEXTS. The type indicates how many payloads it could be encoded in 
      the source side.
   
    Optional parameters:
    :  
        minptime:
        : the minimum interval of such meta data, in milliseconds
        
        maxbitrate:
        : the maximum bitrate of such meta data, in kbps units
      
## Example

~~~~~~~~~~~   
    m=audio 5004 UDP/TLS/RTP/SAVPF 111 104
    c=IN IP4 47.98.240.10
    a=rtcp:9 IN IP4 0.0.0.0
    a=mid:audio
    a=sendrecv
    a=rtcp-mux
    a=rtpmap:111 opus/48000/2
    a=fmtp:111 minptime=10;sprop-stereo=0;stereo=0;usedtx=0;useinbandfec=1
    a=rtpmap:104 annotation/1000
    a=fmtp:104 type=1;maxbitrate=2
~~~~~~~~~~~
{: #sdp-example title="Example SDP Offer with SMART Support" }

In this example, 104 was selected for the media annotation and it will send 
along with audio stream.

## Send Through Data Channel

TODO?

# Synchronize with Media

There are 2 synchornization policis defined, one is no synchornization and they will be 
rendered as fast as they are decoded. For example, the audio transcription should 
be updated even the video was frozen due to network degration.

Sometime the frame needs to be synchornized with video stream, for example, the 
face tracking rectangle, it would be meaningless if the video was not up to date.

In either of the case, the SSRC of the payload is used to find the corresponding 
video where the annotation should be rendered. Sometime, if there is a media relay, 
the corresponding video SSRC chould be CSRC of the receiving stream

The timestamp in the media annotation is the same unit as video RTP timestamp, 
and it needn't to be aligned to a video RTP timestamp. The rendering of media annotation
MUST NOT impact the synchronization of audio and video rendering. It can be an add-on
to the current media rendering.

The media annotation should be displayed in the following video frames until 
the next annotation frame come. If there is no up-coming frame for a long while, 
the application should hide them.

## Render a Media Annotation Frame

When a media annotation frame is decoded, it checks if the timestamp is newer than 
the last rendered video frame. 

If the last video frame is newer, then it should patch the annotation into the rendered 
video frame and redraw the render view. Any video frame after that should be patched 
with the annotation until the clear timer or next annotation frame come.

If the annotation frame is newer, it should be cached in a FIFO queue and once next 
video frame comes, it will check if the annotation frame should be poped and patched
into the video frame.

## RTCWeb implicition

To achieve the synchornized video annotation, RTCWeb needs to open interface for 
application operating the rendering of the annotation with the video. Exposure 
of the video raw data to JS layer and let application renders it with Canvas or 
WebGL would be a good choice.

# Compatible

When the legacy endpoints received such an OFFER, it can just ignore the payload.

# Security Considerations

Such meta data could be privacy as the same level of the media, sender can 
decide to not distribute them. The meta data should be encrypted in the transer 
and it should also support end to end encription.

# IANA Considerations

"application/annotation" media type should be registered as defined in [RFC6838].

# Acknowledgements
